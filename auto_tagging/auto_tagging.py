# auto_tagging.py
import re, os, requests
from typing import List, Set
from collections import Counter

# pip install konlpy
from konlpy.tag import Okt

# pip install scikit-learn
from sklearn.feature_extraction.text import TfidfVectorizer


# ------------------------------
# 0) ì „ì—­ ìºì‹œ (ì„ íƒ ì„±ëŠ¥ ìµœì í™”)
# ------------------------------
_STOPWORDS_CACHE: Set[str] | None = None



# ------------------------------
# 1) í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
# ------------------------------
def clean_text(text: str) -> str:
    # ê°œí–‰ ì œê±° + HTML ì œê±° + íŠ¹ìˆ˜ë¬¸ì ì œê±° + ë‹¤ì¤‘ ê³µë°± ì •ë¦¬
    text = text.replace("\r", " ").replace("\n", " ")
    text = re.sub(r"<[^>]+>", "", text)
    text = re.sub(r"[^ê°€-í£a-zA-Z0-9\s]", " ", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip().lower()


# ------------------------------
# 2) GitHub í•œêµ­ì–´ ë¶ˆìš©ì–´ ë¡œë“œ
# ------------------------------
def get_korean_stopwords_from_github() -> set:
    url = "https://raw.githubusercontent.com/stopwords-iso/stopwords-ko/master/stopwords-ko.txt"
    try:
        r = requests.get(url, timeout=5)
        if r.status_code == 200:
            words = {w.strip() for w in r.text.split("\n") if w.strip()}
            print(f"ğŸŒ GitHub í•œêµ­ì–´ ë¶ˆìš©ì–´ {len(words)}ê°œ ë¶ˆëŸ¬ì˜´")
            return words
    except Exception as e:
        print("âš ï¸ GitHub ë¶ˆìš©ì–´ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨:", e)
    return set()


# ------------------------------
# 3) ë¶ˆìš©ì–´ ë³‘í•© (ê¸°ë³¸ + GitHub + ë¡œì»¬)
# ------------------------------
def load_stopwords() -> set:
    global _STOPWORDS_CACHE
    if _STOPWORDS_CACHE is not None:
        return _STOPWORDS_CACHE

    base_stopwords = {
        # ì¡°ì‚¬/ëŒ€ëª…ì‚¬/í”í•œ ì„œìˆ ì–´
        "ê²ƒ","ìˆ˜","ë“±","ë”","ë°","ë•Œ","ì €","ê·¸","ì´","ê°€","ì€","ëŠ”","ì˜","ì—","ì„","ë¥¼",
        "ì™€","ê³¼","ë„","ìœ¼ë¡œ","í•˜ë‹¤","ìˆë‹¤","ë˜ë‹¤","ê°™ë‹¤","ì•Šë‹¤","ì¢‹ë‹¤","ë§ë‹¤",
        # í”í•œ ì„œìˆ /ì—°ê²°/í˜•ì‹ì–´
        "ì´ë²ˆ","ë•Œë¬¸","ê´€ë ¨","ëŒ€í•œ","ì´í›„","ì´ì „","ê²½ìš°","ìœ„í•´","ì‚¬ìš©","ë°œìƒ","ë¬¸ì œ","ìˆ˜ì¤€","ê¸°ì¤€","ë¶€ë¶„",
        "ìƒí™©","ì ìš©","í†µí•´","ì‘ì—…","ê²°ê³¼","ì§„í–‰","ì‚¬í•­","ìœ¼ë¡œ","ì—ì„œ","ë¶€í„°","ê¹Œì§€",
        "ê·¸ë¦¬ê³ ","í•˜ì§€ë§Œ","ë˜í•œ","ê·¸ë˜ì„œ","ê°™ìŠµë‹ˆë‹¤","ìˆìŠµë‹ˆë‹¤","í•©ë‹ˆë‹¤","ì…ë‹ˆë‹¤",
        # ì¼ë°˜ ëª…ì‚¬(ë¶„ì•¼ ë¬´ê´€í•˜ê²Œ ë„ˆë¬´ ì¼ë°˜ì )
        "ì„œë¹„ìŠ¤","íŒ€","íšŒì‚¬","ì‚¬ìš©ì","ê¸°ëŠ¥","ì •ë³´"
    }

    github_stop = get_korean_stopwords_from_github()

    local_stop = set()
    if os.path.exists("./stopwords.txt"):
        with open("./stopwords.txt", "r", encoding="utf-8") as f:
            local_stop = {line.strip() for line in f if line.strip()}
            print(f"ğŸ“˜ ë¡œì»¬ ë¶ˆìš©ì–´ {len(local_stop)}ê°œ ë¶ˆëŸ¬ì˜´")

    merged = base_stopwords | github_stop | local_stop
    print(f"âœ… ìµœì¢… ë¶ˆìš©ì–´ {len(merged)}ê°œ ì‚¬ìš© ì¤‘")
    _STOPWORDS_CACHE = merged
    return merged


# ------------------------------
# 4) í† í¬ë‚˜ì´ì¦ˆ + í•„í„° (ê³µìš©)
# ------------------------------
def tokenize_and_filter(text: str, stopwords: set) -> List[str]:
    """
    - Okt ëª…ì‚¬ ì¶”ì¶œ
    - ë¶ˆìš©ì–´ ì œê±°
    - í•œ ê¸€ì ì œê±°
    - ì˜ë¯¸ ë¹ˆì•½ ì ‘ë¯¸(í•¨/ë¨/ì„±) ì œê±°
    - ìˆ«ì ë‹¨ë…(ì„ íƒ) ì œê±°
    """
    okt = Okt()
    text = clean_text(text)
    nouns = okt.nouns(text)

    tokens = []
    for n in nouns:
        if len(n) <= 1:
            continue
        if n in stopwords:
            continue
        if n.endswith(("í•¨", "ë¨", "ì„±")):  # ì˜ˆ: ë°œìƒ -> 'ë°œìƒ'ì€ base_stopwordsì— ìˆì§€ë§Œ, 'ë°œìƒë¨' ê°™ì€ ë¥˜ ë°©ì§€
            continue
        if n.isdigit():
            continue
        tokens.append(n)
    return tokens


# ------------------------------
# 5) ë¹ˆë„ ê¸°ë°˜ í‚¤ì›Œë“œ
# ------------------------------
def extract_keywords(text: str, top_n: int = 10, stopwords: set | None = None) -> List[str]:
    stopwords = stopwords or load_stopwords()
    tokens = tokenize_and_filter(text, stopwords)
    count = Counter(tokens)
    return [w for w, _ in count.most_common(top_n)]


# ------------------------------
# 6) TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ (ë¶ˆìš©ì–´ ì ìš©)
# ------------------------------
def extract_keywords_tfidf(text: str, top_n: int = 10, stopwords: set | None = None) -> List[str]:
    """
    í•œêµ­ì–´ TF-IDFëŠ” ê¸°ë³¸ í† í¬ë‚˜ì´ì €ê°€ ë¶€ì í•©í•˜ë¯€ë¡œ,
    â‘  Oktë¡œ ì‚¬ì „ í† í¬ë‚˜ì´ì¦ˆ â†’ â‘¡ ë¶ˆìš©ì–´/í•„í„° â†’ â‘¢ ê³µë°±-ì¡°ì¸ â†’ â‘£ ë²¡í„°ë¼ì´ì €
    """

    stopwords = stopwords or load_stopwords()
    tokens = tokenize_and_filter(text, stopwords)
    pretokenized = " ".join(tokens)

    vectorizer = TfidfVectorizer(
        max_features=2000,
        tokenizer=None,          # ë¯¸ì‚¬ìš© (ì´ë¯¸ ê³µë°± í† í°í™”ë¨)
        token_pattern=r"(?u)\b\w+\b",  # í† í°ì€ ê³µë°± ë¶„ë¦¬ë¡œ ë“¤ì–´ì˜¤ë¯€ë¡œ ëª¨ë“  ë‹¨ì–´ í—ˆìš©
        lowercase=False          # ì´ë¯¸ ì •ê·œí™”ë¨
    )
    X = vectorizer.fit_transform([pretokenized])
    scores = X.toarray()[0]
    terms = vectorizer.get_feature_names_out()

    top_idx = scores.argsort()[-top_n:][::-1]
    return [terms[i] for i in top_idx]


# ------------------------------
# 7) ìë™ íƒœê¹… (ì œëª© ê°€ì¤‘ì¹˜ ì˜µì…˜)
# ------------------------------
def auto_tag_post(title: str, content: str, use_tfidf: bool = False, title_boost: bool = True) -> List[str]:
    stop = load_stopwords()

    # ì œëª© ê°€ì¤‘ì¹˜: ì œëª©ì„ ë³¸ë¬¸ ì•ì— 2ë²ˆ ë” ë¶™ì—¬ì„œ(=ê°€ì¤‘ì¹˜â†‘) ì¤‘ìš” ë‹¨ì–´ê°€ ìƒìœ„ë¡œ ì˜¤ë„ë¡
    text = (f"{title} " * (3 if title_boost else 1)) + content

    tags = (
        extract_keywords_tfidf(text, top_n=8, stopwords=stop)
        if use_tfidf
        else extract_keywords(text, top_n=8, stopwords=stop)
    )
    # ì›í˜• ë³´ì¡´ ì •ë ¬
    return sorted(set(tags), key=tags.index)



# ------------------------------
# 8) ì…ë ¥
# ------------------------------
def get_post_content():
    print("â—ï¸â—ï¸input.txtì— ë¯¸ë¦¬ ë³µë¶™í•´ë‘ì‹œëŠ” ê²ƒì„ ê°•ë ¥ ì¶”ì²œë“œë¦½ë‹ˆë‹¤ -> ì¶”í›„ 2ë²ˆ ì„ íƒâ—ï¸â—ï¸")
    choice = input("ğŸ“‚ í…ìŠ¤íŠ¸ ì…ë ¥ ë°©ì‹ ì„ íƒ (1: ì§ì ‘ ì…ë ¥ / 2: txt íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°): ").strip()
    if choice == "1":
        return input("\nâœï¸ í¬ìŠ¤íŠ¸ ë‚´ìš©ì„ í•œ ì¤„ë¡œ ì…ë ¥í•˜ì„¸ìš”:\n> ")
    elif choice == "2":
        file_path = input("\nğŸ“„ ë¶ˆëŸ¬ì˜¬ txt íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸: ./input.txt): ").strip() or "./input.txt"
        if file_path == "./input.txt":
            print(f"ğŸ“‚ ê¸°ë³¸ íŒŒì¼ ê²½ë¡œë¡œ ì§„í–‰í•©ë‹ˆë‹¤: {file_path}")
        if not os.path.exists(file_path):
            print("ğŸš« íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return ""
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read().replace("\r", " ").replace("\n", " ")
    else:
        print("âš ï¸ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. ê¸°ë³¸ê°’(ë¹ˆ ë‚´ìš©)ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
        return ""
    

# ------------------------------
# 9) ì €ì¥
# ------------------------------
def save_tags_to_file(tags: List[str], title: str, out_dir: str = "tag_result"):
    os.makedirs(out_dir, exist_ok=True)
    safe_title = re.sub(r"[^ê°€-í£a-zA-Z0-9]", "_", title.strip())
    output_path = os.path.join(out_dir, f"tag_result_{safe_title}.txt")

    with open(output_path, "w", encoding="utf-8") as f:
        f.write(f"âœ… ì œëª©: {title}\n\n")
        f.write("âœ… ìë™ ìƒì„±ëœ íƒœê·¸ ëª©ë¡\n\n")
        for tag in tags:
            f.write(f"- {tag}\n")

    print(f"\nğŸ’¾ íƒœê·¸ê°€ '{output_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")


# ------------------------------
# 10) ì‹¤í–‰
# ------------------------------
if __name__ == "__main__":
    print("ğŸš€ Auto Tagging ì‹œì‘!")
    title = input("ğŸ“ í¬ìŠ¤íŠ¸ ì œëª©ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    content = get_post_content()
    if not content:
        print("âŒ ë³¸ë¬¸ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        raise SystemExit

    mode = input("\nâš™ï¸ TF-IDF ê¸°ë°˜ ë¶„ì„ì„ ì‚¬ìš©í• ê¹Œìš”? (y/n): ").strip().lower()
    use_tfidf = (mode == "y")

    tags = auto_tag_post(title, content, use_tfidf=use_tfidf, title_boost=True)
    print("\nâœ… ìë™ ìƒì„±ëœ íƒœê·¸:")
    print(", ".join(tags))
    save_tags_to_file(tags, title)